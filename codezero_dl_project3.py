# -*- coding: utf-8 -*-
"""CodeZero_DL_Project3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UklwJ91O3evWO50wKIGt0c9QSE9xzI5l

# Deep Learning: Final Project

## Setup & Imports
"""

!pip install torch torchvision numpy matplotlib pandas
!pip install torchattacks

import torchvision
import numpy as np
import torch
from torch.utils.data import DataLoader
import json
import os
import torch.nn.functional as F
import matplotlib.pyplot as plt
import torchvision.transforms.functional as TF
import pandas as pd

"""### Download pre-trained model"""

pretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')

"""### Data Loading & Normalization"""

!unzip -n TestDataSet.zip

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

mean_norms = np.array([0.485, 0.456, 0.406])
std_norms = np.array([0.229, 0.224, 0.225])
plain_transforms = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=mean_norms, std=std_norms)
])

batch_size = 128
dataset_path = "./TestDataSet"

orig_dataset = torchvision.datasets.ImageFolder(root=dataset_path, transform=plain_transforms)
orig_dataloader = DataLoader(orig_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

# Load label mapping
with open(dataset_path + '/labels_list.json', "r") as f:
    label_list = json.load(f)
class_idx = [int(label.split(': ')[0]) for label in label_list]
class_label = [label.split(': ')[1] for label in label_list]

# map imagenet index to labels
imagenet_class_mapping = {idx: label for idx, label in zip(class_idx, class_label)}

# map dataset target to imagenet target
label_to_imagenet = {target: idx for target, idx in enumerate(class_idx)}

def denormalize(image, mean_norms=mean_norms, std_norms=std_norms):
    """denormalize image tensor using mean/std"""
    mean = torch.tensor(mean_norms).view(1, 3, 1, 1).to(image.device)
    std = torch.tensor(std_norms).view(1, 3, 1, 1).to(image.device)
    return torch.clamp(image * std + mean, 0, 1)

"""## Task 1: Evaluate pre-trained model"""

def evaluate(model, dataloader, save_preds=False, col_name=None, filepath='pred.csv'):
    model = model.to(device)
    model.eval()

    top1_correct = 0
    top5_correct = 0
    total = 0
    preds = []
    preds_top5 = []

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device)
            imagenet_labels = torch.tensor(
                [label_to_imagenet[int(l)] for l in labels], device=device
            )

            outputs = model(images)
            _, top5_preds = outputs.topk(5, dim=1)

            # Top-1 accuracy
            top1_correct += (top5_preds[:, 0] == imagenet_labels).sum().item()
            preds.extend(top5_preds[:, 0].cpu().numpy())

            # Top-5 accuracy
            for i in range(images.size(0)):
                if imagenet_labels[i].item() in top5_preds[i]:
                    top5_correct += 1
            preds_top5.extend(top5_preds.cpu().numpy().tolist())

            total += images.size(0)

    top1_acc = top1_correct / total
    top5_acc = top5_correct / total

    # save predictions in csv if needed
    if save_preds:
        if not col_name:
            raise ValueError('Expected column name')
        if os.path.isfile(filepath):
            df = pd.read_csv(filepath)
            df[col_name] = preds
            df[col_name + '_top5'] = preds_top5
        else:
            df = pd.DataFrame({col_name: preds, col_name + '_top5': preds_top5})
        df.to_csv(filepath, index=False)

    print(f"Top-1 Accuracy: {top1_acc * 100:.2f}%")
    print(f"Top-5 Accuracy: {top5_acc * 100:.2f}%")
    return preds

orig_preds = evaluate(pretrained_model, orig_dataloader, save_preds=True, col_name='orig')

"""### Writeup: Task 1: Evaluate Pre-trained Model

We evaluated the pre-trained ResNet-34 model on the provided ImageNet-1K subset test dataset. The images were normalized using the given mean and standard deviation:

- Mean: [0.485, 0.456, 0.406]
- Std: [0.229, 0.224, 0.225]

The ground truth labels from the dataset folders were mapped to ImageNet label indices using the provided `.json` file. We computed both top-1 and top-5 accuracy by comparing predictions to these label indices.

Final evaluation results:
- Top-1 Accuracy: 76.00%
- Top-5 Accuracy: 94.20%

## Task 2: Pixel-wise attacks

https://pytorch.org/tutorials/beginner/fgsm_tutorial.html
"""

fgsm_output_dir = "./AdversarialTestSet1"
os.makedirs(fgsm_output_dir, exist_ok=True)

adv_dataloader = torch.utils.data.DataLoader(orig_dataset, batch_size=1, shuffle=False)
epsilon = 0.02 / std_norms
epsilon_tensor = torch.tensor(epsilon).view(1, 3, 1, 1).to(device, dtype=torch.float32)

# FGSM attack
def fgsm_attack(image, label, model, epsilon_tensor):
    image.requires_grad = True
    output = model(image)
    loss = F.cross_entropy(output, label)
    model.zero_grad()
    loss.backward()
    data_grad = image.grad.data

    sign_data_grad = data_grad.sign()
    perturbed_image = image + epsilon_tensor * sign_data_grad
    min_vals = torch.tensor((0 - mean_norms) / std_norms, dtype=torch.float32, device=image.device).view(1, 3, 1, 1)
    max_vals = torch.tensor((1 - mean_norms) / std_norms, dtype=torch.float32, device=image.device).view(1, 3, 1, 1)

    return torch.clamp(perturbed_image, min=min_vals, max=max_vals).to(dtype=torch.float32)

# Save images
from pathlib import Path

# Get original image file paths from the dataset
original_paths = [Path(orig_dataset.imgs[i][0]) for i in range(len(orig_dataset))]

# Attack loop
orig_images = []
fgsm_images = []

for i, (image, label) in enumerate(adv_dataloader):
    image, label = image.to(device), label.to(device)
    # map targets from test dataset to imagenet predictions
    imagenet_label = torch.tensor([label_to_imagenet[int(label.item())]], device=device)
    fgsm_image = fgsm_attack(image.clone(), imagenet_label, pretrained_model, epsilon_tensor)
    orig_images.append(image.squeeze(0).cpu())
    fgsm_images.append(fgsm_image.squeeze(0).cpu())

    # Determine relative path inside dataset
    orig_path = original_paths[i]
    rel_path = orig_path.relative_to(orig_dataset.root)
    save_path = Path(fgsm_output_dir) / rel_path

    # Make directory if needed
    save_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert and save
    img_denorm = denormalize(fgsm_image).squeeze(0)
    pil_img = TF.to_pil_image(img_denorm)
    pil_img.save(save_path)

"""### Get FGSM accuracy drop"""

fgsm_dataset = torchvision.datasets.ImageFolder(root=fgsm_output_dir, transform=plain_transforms)
fgsm_dataloader = DataLoader(fgsm_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
fgsm_preds = evaluate(pretrained_model, fgsm_dataloader, save_preds=True, col_name='fgsm')

"""### Verify L_inf"""

# Report L_inf statistics
def get_max_linf(orig_images, adv_images):
    # assert len(orig_dataset) == len(adv_dataset), "Datasets must have the same length"
    assert len(orig_dataset) == len(orig_images)

    max_diff = 0
    for orig_image, adv_image in zip(orig_images, adv_images):
        denorm_orig_image = denormalize(orig_image)
        denorm_adv_image = denormalize(adv_image)
        diff = torch.abs(denorm_orig_image - denorm_adv_image)
        max_diff = max(max_diff, diff.max().item())
    print(rf"Max L_inf difference: {max_diff:.5f}")

get_max_linf(orig_images, fgsm_images)

"""### Get Successful Attack Samples"""

import random

def show_successful_attacks(indices, model, orig_dataset, adv_dataset):
    model.eval()

    for i in indices:
        # Load normalized images and label
        orig_img, label1 = orig_dataset[i]
        adv_img, _ = adv_dataset[i]
        label = label1

        # Run model
        with torch.no_grad():
            orig_pred = model(orig_img.unsqueeze(0).to(device)).argmax(dim=1).item()
            adv_pred = model(adv_img.unsqueeze(0).to(device)).argmax(dim=1).item()

        # Denormalize for visualization
        orig_vis = denormalize(orig_img.unsqueeze(0)).squeeze(0).cpu()
        adv_vis = denormalize(adv_img.unsqueeze(0)).squeeze(0).cpu()

        # Plot
        fig, axs = plt.subplots(1, 2, figsize=(8, 4))
        fig.suptitle(f"True Label: {label_to_imagenet[label]}")
        axs[0].imshow(TF.to_pil_image(orig_vis))
        axs[0].set_title(f"Original: {orig_pred}")
        axs[1].imshow(TF.to_pil_image(adv_vis))
        axs[1].set_title(f"Attack: {adv_pred}")
        for ax in axs: ax.axis('off')
        plt.tight_layout()
        plt.show()

successful_attacks = []
df = pd.read_csv('pred.csv')

for idx, row in df.iterrows():
    img, label = orig_dataset[idx]
    if label_to_imagenet[label] == row['orig'] and row['orig'] != row['fgsm']:
        successful_attacks.append(idx)

# Show 5 random successful attacks
selected = random.sample(successful_attacks, min(5, len(successful_attacks)))
show_successful_attacks(selected, pretrained_model, orig_dataset, fgsm_dataset)

"""### Writeup: Task 2: Pixel-wise Attacks (FGSM, L∞)

We implemented the Fast Gradient Sign Method (FGSM) attack with an L∞ constraint of ε = 0.02. This value corresponds to a maximum pixel-level change of ±1 in raw (unprocessed) image space. Gradients were computed with respect to the input image, and a single gradient ascent step was applied in the direction of the sign of the gradient.

Each perturbed image was clipped to stay within the valid range after normalization. The resulting adversarial examples were visually inspected and confirmed to be perceptually similar to the originals.

- A total of 500 adversarial images were generated and saved as **Adversarial Test Set 1**.
- We verified that the L∞ difference between original and adversarial images was **exactly 0.02000**.
- 3–5 examples were visualized where the attack caused misclassification.

Evaluation of the ResNet-34 model on the adversarial dataset yielded:
- **Top-1 Accuracy:** 6.00%
- **Top-5 Accuracy:** 31.20%

This reflects a drop of over 50% in both metrics compared to baseline performance from Task 1 (Top-1: 76.00%, Top-5: 94.20%), as required.

## Task 3: Improved Attacks

### Projected Gradient Descent
"""

def pgd_attack(model, images, labels, epsilon, alpha, num_iter, mean_norms=mean_norms, std_norms=std_norms):
    """
    PGD attack under L∞ norm.

    Args:
        model: pretrained model
        images: normalized input images (B, C, H, W)
        labels: correct labels (mapped to ImageNet indices)
        epsilon: max perturbation (same shape as image or broadcastable)
        alpha: step size (same shape as epsilon)
        num_iter: number of attack steps
        mean_norms, std_norms: normalization used in preprocessing

    Returns:
        perturbed images (still normalized)
    """
    ori_images = images.clone().detach()
    perturbed = images.clone().detach().requires_grad_(True)

    for _ in range(num_iter):
        outputs = model(perturbed)
        model.zero_grad()
        loss = F.cross_entropy(outputs, labels)
        loss.backward()

        # Gradient sign
        grad_sign = perturbed.grad.data.sign()

        # Apply perturbation and clip to epsilon
        perturbed = perturbed + alpha * grad_sign
        delta = torch.clamp(perturbed - ori_images, min=-epsilon, max=epsilon)

        min_vals = torch.tensor((0 - mean_norms) / std_norms, dtype=torch.float32, device=image.device).view(1, 3, 1, 1)
        max_vals = torch.tensor((1 - mean_norms) / std_norms, dtype=torch.float32, device=image.device).view(1, 3, 1, 1)
        perturbed = torch.clamp(ori_images + delta, min=min_vals, max=max_vals).to(torch.float32)
        perturbed = perturbed.detach().requires_grad_(True)

    return perturbed

pgd_output_dir = "./AdversarialTestSet2"

alpha = epsilon / 4
alpha_tensor = torch.tensor(alpha).view(1, 3, 1, 1).to(device, dtype=torch.float32)

pgd_images = []

pretrained_model.eval()
for i, (image, label) in enumerate(adv_dataloader):
    image, label = image.to(device), label.to(device)
    imagenet_label = torch.tensor([label_to_imagenet[int(label.item())]], device=device)
    pgd_img = pgd_attack(pretrained_model, image, imagenet_label, epsilon_tensor, alpha_tensor, num_iter=10)
    pgd_images.append(pgd_img.cpu())

# Get original image file paths from the dataset
original_paths = [Path(orig_dataset.imgs[i][0]) for i in range(len(orig_dataset))]

# Save loop
for i, (img, orig_path) in enumerate(zip(pgd_images, original_paths)):
    # Determine relative path inside dataset
    rel_path = orig_path.relative_to(orig_dataset.root)
    save_path = Path(pgd_output_dir) / rel_path

    # Make directory if needed
    save_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert and save
    img_denorm = denormalize(img).squeeze(0)
    pil_img = TF.to_pil_image(img_denorm)
    pil_img.save(save_path)

pgd_dataset = torchvision.datasets.ImageFolder(root=pgd_output_dir, transform=plain_transforms)
pgd_dataloader = DataLoader(pgd_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
pgd_preds = evaluate(pretrained_model, pgd_dataloader, save_preds=True, col_name='pgd')

get_max_linf(orig_images, pgd_images)

successful_attacks = []
df = pd.read_csv('pred.csv')

for idx, row in df.iterrows():
    img, label = orig_dataset[idx]
    if label_to_imagenet[label] == row['orig'] and \
       row['orig'] == row['fgsm'] and \
       row['fgsm'] != row['pgd']:
        successful_attacks.append(idx)

# Show 5 random successful attacks
selected = random.sample(successful_attacks, min(5, len(successful_attacks)))
show_successful_attacks(selected, pretrained_model, orig_dataset, pgd_dataset)

"""### Auto-PGD"""

def apgd_attack(model, image, label, epsilon_tensor, alpha_tensor, num_iter,
                mean_norms=mean_norms, std_norms=std_norms):
    """
    APGD attack

    Args:
        model: pretrained model
        images: normalized input images (B, C, H, W)
        labels: correct labels (ImageNet indices)
        epsilon: max perturbation (same shape as image or broadcastable)
        alpha: step size (same shape as epsilon)
        num_iter: number of steps
        mean_norms, std_norms: used for preprocessing
        loss_fn: 'ce' (cross-entropy) or 'dlr' (difference of logits ratio)

    Returns:
        perturbed images (still normalized)
    """
    ori_images = image.clone().detach()
    mean = torch.tensor(mean_norms, dtype=torch.float32, device=image.device).view(1, 3, 1, 1)
    std = torch.tensor(std_norms, dtype=torch.float32, device=image.device).view(1, 3, 1, 1)
    perturbed = ori_images + torch.empty_like(image).uniform_(-1.0, 1.0) * epsilon_tensor
    perturbed = perturbed.clamp((0 - mean) / std, (1 - mean) / std).detach().requires_grad_(True)

    best_adv = perturbed.clone().detach()
    best_loss = torch.full((image.size(0),), -float('inf'), device=device)

    for _ in range(num_iter):
        output = model(perturbed)
        loss = F.cross_entropy(output, label, reduction='none')
        grad = torch.autograd.grad(loss.sum(), perturbed)[0]
        perturbed = perturbed + alpha_tensor * grad.sign()
        delta = torch.clamp(perturbed - ori_images, -epsilon_tensor, epsilon_tensor)

        min_vals = torch.tensor((0 - mean_norms) / std_norms, dtype=torch.float32, device=image.device).view(1, 3, 1, 1)
        max_vals = torch.tensor((1 - mean_norms) / std_norms, dtype=torch.float32, device=image.device).view(1, 3, 1, 1)
        perturbed = torch.clamp(ori_images + delta, min_vals, max_vals).detach().requires_grad_(True)
        perturbed = perturbed.to(dtype=torch.float32)

        with torch.no_grad():
            outputs = model(perturbed)
            cur_loss = F.cross_entropy(outputs, label, reduction='none')
            mask = cur_loss > best_loss
            best_loss[mask] = cur_loss[mask]
            best_adv[mask] = perturbed[mask]

    return best_adv.detach()

apgd_output_dir = "./AdversarialTestSet3"

apgd_images = []

pretrained_model.eval()
for i, (image, label) in enumerate(adv_dataloader):
    image, label = image.to(device), label.to(device)
    imagenet_label = torch.tensor([label_to_imagenet[int(label.item())]], device=device)
    apgd_img = apgd_attack(pretrained_model, image, imagenet_label, epsilon_tensor, alpha_tensor, 10)
    apgd_images.append(apgd_img.cpu())

# Save loop
for i, (img, orig_path) in enumerate(zip(apgd_images, original_paths)):
    # Determine relative path inside dataset
    rel_path = orig_path.relative_to(orig_dataset.root)
    save_path = Path(apgd_output_dir) / rel_path

    # Make directory if needed
    save_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert and save
    img_denorm = denormalize(img).squeeze(0)
    pil_img = TF.to_pil_image(img_denorm)
    pil_img.save(save_path)

apgd_dataset = torchvision.datasets.ImageFolder(root=apgd_output_dir, transform=plain_transforms)
apgd_dataloader = DataLoader(apgd_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
apgd_preds = evaluate(pretrained_model, apgd_dataloader, save_preds=True, col_name='apgd')

get_max_linf(orig_images, apgd_images)

"""### Gaussian Noise Attack"""

import torchattacks                   # https://adversarial-attacks-pytorch.readthedocs.io
gn_output_dir = "./AdversarialTestSet_GN"
os.makedirs(gn_output_dir, exist_ok=True)

#  Build the attacker.  'std' is BEFORE normalisation, so convert:
raw_std = 0.02                        # ≈1 raw‑pixel step (same budget as FGSM)
norm_std = raw_std / std_norms.mean() # quick heuristic: divide by mean σ
gn_attack = torchattacks.GN(pretrained_model, std=norm_std)

#  Generate & save perturbed images (mirrors ImageFolder layout)
gn_images, gn_targets = [], []        # keep in‑RAM copy for metrics
pretrained_model.eval()

for (img, target), orig_path in zip(adv_dataloader, original_paths):
    img, target = img.to(device), target.to(device)
    imagenet_target = torch.tensor([label_to_imagenet[int(target.item())]], device=device)

    with torch.no_grad():             # GN doesn’t use gradients
        adv_img = gn_attack(img, imagenet_target)

    # *** Clip to L∞ ≤ ε ***   (GN may exceed the budget on rare pixels)
    delta     = torch.clamp(adv_img - img, min=-epsilon_tensor, max=epsilon_tensor)
    adv_img   = img + delta

    # save PNG
    rel_path  = orig_path.relative_to(orig_dataset.root)
    save_path = Path(gn_output_dir)/rel_path
    save_path.parent.mkdir(parents=True, exist_ok=True)
    TF.to_pil_image(denormalize(adv_img).squeeze(0)).save(save_path)

    # bookkeeping
    gn_images .append(adv_img.squeeze(0).cpu())
    gn_targets.append(imagenet_target.cpu())

print("Gaussian Noise images written to", gn_output_dir)

#  Wrap in DataLoader and evaluate
gn_dataset   = torchvision.datasets.ImageFolder(root=gn_output_dir,
                                                transform=plain_transforms)
gn_loader    = DataLoader(gn_dataset, batch_size=batch_size,
                          shuffle=False, num_workers=2)

print("\nResNet‑34 on Gaussian‑Noise set")
gn_preds = evaluate(pretrained_model, gn_loader, save_preds=True, col_name='gn')

#  Verify budget
get_max_linf(orig_images, gn_images)

"""### Jitter Attack"""

import torchattacks, types, math
from pathlib import Path


# 1) Wrap the ResNet so the attacker can work in RAW [0,1] space
class RawWrapper(torch.nn.Module):
    """Takes raw images in [0,1], normalises internally, then calls the model."""
    def __init__(self, model, mean, std):
        super().__init__()
        self.model = model
        self.register_buffer('mean', torch.tensor(mean).view(1,3,1,1))
        self.register_buffer('std' , torch.tensor(std ).view(1,3,1,1))
    def forward(self, x):
        x_norm = (x - self.mean) / self.std
        return self.model(x_norm)

wrapper = RawWrapper(pretrained_model, mean_norms.astype('float32'),
                     std_norms.astype('float32')).to(device).eval()

# 2) Instantiate Jitter with our ε / α in RAW space
raw_eps   = 0.02                    # identical budget to FGSM (~5.1/255)
raw_alpha = 0.005                   # a sane step (~1.3/255)
jit = torchattacks.Jitter(wrapper, eps=raw_eps, alpha=raw_alpha,
                          steps=10, scale=10, std=0.1, random_start=True)

# 3) Build adversarial set
jitter_dir = Path("./AdversarialTestSet_Jitter")
jitter_dir.mkdir(exist_ok=True)

jitter_imgs_norm, jitter_tgts = [], []   # store for L∞ check
mean = torch.tensor(mean_norms, dtype=torch.float32, device=device).view(1,3,1,1)
std  = torch.tensor(std_norms , dtype=torch.float32, device=device).view(1,3,1,1)

for (img_norm, target), orig_path in zip(adv_dataloader, original_paths):
    img_norm, target = img_norm.to(device), target.to(device)
    img_raw  = torch.clamp(img_norm * std + mean, 0, 1)

    tgt_imagenet = torch.tensor([label_to_imagenet[int(target.item())]], device=device)
    adv_raw = jit(img_raw, tgt_imagenet)           # returns RAW tensor ∈[0,1]

    # convert back to normalised space for evaluation / ∞‑budget check
    adv_norm = (adv_raw - mean) / std
    jitter_imgs_norm.append(adv_norm.squeeze(0).cpu())
    jitter_tgts     .append(tgt_imagenet.cpu())

    # save PNG (RAW) mirroring original folder tree
    rel_path = orig_path.relative_to(orig_dataset.root)
    save_path = jitter_dir / rel_path
    save_path.parent.mkdir(parents=True, exist_ok=True)
    TF.to_pil_image(adv_raw.squeeze(0).cpu()).save(save_path)

print("Jitter images written:", len(jitter_imgs_norm))

# 4) Evaluate ResNet‑34 on Jitter set
jitter_ds   = torchvision.datasets.ImageFolder(root=str(jitter_dir),
                                               transform=plain_transforms)
jitter_dl   = DataLoader(jitter_ds, batch_size=batch_size,
                         shuffle=False, num_workers=2)

print("\nResNet‑34 on Jitter set")
jitter_preds = evaluate(pretrained_model, jitter_dl, save_preds=True, col_name='jitter')

# 5) Verify ‖·‖∞ ≤ ε in RAW space
max_linf = 0.0
for o,a in zip(orig_images, jitter_imgs_norm):
    diff = torch.abs(denormalize(o.unsqueeze(0)) - denormalize(a.unsqueeze(0)))
    max_linf = max(max_linf, diff.max().item())
print(f"Max L_inf difference: {max_linf:.5f}")

"""### Writeup: Task 3: Improved Attacks

We implemented two improved adversarial attack methods under the L∞ norm with ε = 0.02:

1. **Projected Gradient Descent (PGD)**:
   - Applied iterative gradient sign updates with α = ε/4 over 10 steps.
   - Each update was clipped to stay within the ε-ball and valid image range.
   - 500 adversarial images were saved as **Adversarial Test Set 2**.
   - Verified that the L∞ distance from original images was exactly 0.02000.
   - ResNet-34 evaluation:
     - Top-1 Accuracy: 1.00%
     - Top-5 Accuracy: 31.20%

2. **Auto-PGD (APGD)**:
   - An advanced attack with adaptive step selection and best-image tracking across 10 steps.
   - Same ε and α constraints were enforced.
   - Saved 500 adversarial images as **Adversarial Test Set 3**.
   - Verified L∞ constraint: 0.02000.
   - ResNet-34 evaluation:
     - Top-1 Accuracy: 4.20%
     - Top-5 Accuracy: 43.40%

Both PGD and APGD outperformed the baseline FGSM attack. The PGD attack achieved a top-1 accuracy drop of 75 percentage points from the original (76.00%), exceeding the required 70% drop. Adversarial examples were also visualized to confirm successful misclassification.

## Task 4: Patch Attacks
"""

def patch_pgd_attack(model, image, label, epsilon, alpha, num_iter,
                     mean_norms=mean_norms, std_norms=std_norms, patch_size=32):
    """
    PGD attack applied only to a patch of each image.

    Args:
        model: pretrained model
        images: normalized input images (B, C, H, W)
        labels: correct labels
        epsilon, alpha: scalar or broadcastable tensors
        num_iter: number of PGD steps
        mean_norms, std_norms: normalization used
        patch_size: square patch size to perturb

    Returns:
        Perturbed images (still normalized)
    """

    B, C, H, W = image.shape

    ori_image = image.clone().detach()
    perturbed = ori_image.clone().detach().requires_grad_(True)

    # Create patch mask
    patch_masks = torch.zeros_like(ori_image)
    top = (H - patch_size) // 2
    left = (W - patch_size) // 2
    patch_masks[:, :, top:top+patch_size, left:left+patch_size] = 1.0


    for _ in range(num_iter):
        output = model(perturbed)
        loss = F.cross_entropy(output, label)
        model.zero_grad()
        loss.backward()

        grad_sign = perturbed.grad.data.sign()

        # Restrict update to patch only
        perturb_update = alpha * grad_sign * patch_masks
        perturbed = perturbed + perturb_update

        # Restrict delta and clip
        delta = torch.clamp(perturbed - ori_image, min=-epsilon, max=epsilon)
        delta = delta * patch_masks  # Only retain deltas in the patch

        min_vals = torch.tensor((0 - mean_norms) / std_norms, dtype=torch.float32, device=device).view(1, 3, 1, 1)
        max_vals = torch.tensor((1 - mean_norms) / std_norms, dtype=torch.float32, device=device).view(1, 3, 1, 1)
        perturbed = torch.clamp(ori_image + delta, min=min_vals, max=max_vals).detach().requires_grad_(True)

    return perturbed

patch_pgd_output_dir = "./AdversarialTestSet4"

patch_pgd_images = []

patch_epsilon = 0.3 / std_norms
patch_epsilon_tensor = torch.tensor(patch_epsilon, dtype=torch.float32, device=device).view(1, 3, 1, 1)
patch_alpha = patch_epsilon / 10
patch_alpha_tensor = torch.tensor(patch_alpha).view(1, 3, 1, 1).to(device, dtype=torch.float32)

pretrained_model.eval()
for i, (image, label) in enumerate(adv_dataloader):
    image, label = image.to(device), label.to(device)
    imagenet_label = torch.tensor([label_to_imagenet[int(label.item())]], device=device)
    patch_pgd_img = patch_pgd_attack(pretrained_model, image, imagenet_label, patch_epsilon_tensor, patch_alpha_tensor, 200)
    patch_pgd_images.append(patch_pgd_img.cpu())

# Save loop
for i, (img, orig_path) in enumerate(zip(patch_pgd_images, original_paths)):
    # Determine relative path inside dataset
    rel_path = orig_path.relative_to(orig_dataset.root)
    save_path = Path(patch_pgd_output_dir) / rel_path

    # Make directory if needed
    save_path.parent.mkdir(parents=True, exist_ok=True)

    # Convert and save
    img_denorm = denormalize(img).squeeze(0)
    pil_img = TF.to_pil_image(img_denorm)
    pil_img.save(save_path)

patch_pgd_dataset = torchvision.datasets.ImageFolder(root=patch_pgd_output_dir, transform=plain_transforms)
patch_pgd_dataloader = DataLoader(patch_pgd_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
patch_pgd_preds = evaluate(pretrained_model, patch_pgd_dataloader, save_preds=True, col_name='patch_pgd')

"""### Writeup: Task 4: Patch Attacks

We implemented a patch-based PGD attack where only a 32×32 region in each image was perturbed. This region was placed at the center of the image. Since the attack is restricted to a small area, we increased the L∞ perturbation budget to ε = 0.3 and used a step size of α = 0.03. The attack was run for 200 iterations per image.

A binary patch mask was created to restrict gradient updates only to the designated patch. Perturbation was clipped to lie within the ε-ball and valid normalized image bounds.

- 500 perturbed images were generated and saved as **Adversarial Test Set 3**.
- 3–5 visualizations were included showing examples of successful misclassification.
- Evaluation of ResNet-34 on this dataset resulted in:
  - **Top-1 Accuracy:** 8.20%
  - **Top-5 Accuracy:** 47.40%

This confirms that the patch-constrained adversarial attack caused substantial performance degradation, despite modifying only a limited region in each image.

## Task 5: Transferring Attacks
"""

new_model = torchvision.models.efficientnet_v2_s(weights='IMAGENET1K_V1')

dataloaders = {
    'original': orig_dataloader,
    'fgsm': fgsm_dataloader,
    'pgd': pgd_dataloader,
    'apgd': apgd_dataloader,
    'patch_pgd': patch_pgd_dataloader
}

for name, dataloader in dataloaders.items():
    print(f'Evaluating {name} dataset')
    evaluate(new_model, dataloader)
    print()

paths = {
  'Original': './TestDataSet',
  'FGSM':      './AdversarialTestSet1',
  'PGD':       './AdversarialTestSet2',
  'Patch':     './AdversarialTestSet3'
}
transform = plain_transforms

# 1) Visual examples: three random images
def denorm(img):
    arr = img.cpu().numpy().transpose(1,2,0)
    return np.clip(arr * std_norms + mean_norms, 0, 1)

orig_ds = torchvision.datasets.ImageFolder(root=paths['Original'], transform=transform)
adv_ds  = {
    k: torchvision.datasets.ImageFolder(root=v, transform=transform)
    for k, v in paths.items() if k != 'Original'
}

idxs = random.sample(range(len(orig_ds)), 3)
fig, axes = plt.subplots(len(idxs), len(paths), figsize=(12, 9))

for r, idx in enumerate(idxs):
    # Original
    img_o, lbl = orig_ds[idx]
    axes[r, 0].imshow(denorm(img_o))
    axes[r, 0].set_title(f"Original\nLabel={lbl}")
    axes[r, 0].axis('off')
    # Adversarial
    for c, name in enumerate(['FGSM','PGD','Patch'], start=1):
      img_a, _ = adv_ds[name][idx]
      axes[r, c].imshow(denorm(img_a))
      axes[r, c].set_title(name)
      axes[r, c].axis('off')

plt.tight_layout()
plt.show()

# Cell: Summary Table (Top-1 and Top-5)
import pandas as pd
import numpy as np

df = pd.read_csv('pred.csv')
imagenet_labels = [label_to_imagenet[int(label)] for _, label in orig_dataset.imgs]
imagenet_labels = np.array(imagenet_labels)

def top5_acc(preds, labels):
    return np.mean([l in row for l, row in zip(labels, preds)])

attacks = ['orig', 'fgsm', 'pgd', 'apgd', 'patch_pgd', 'gn', 'jitter']
attack_names = {
    'orig': 'Original',
    'fgsm': 'FGSM',
    'pgd': 'PGD',
    'apgd': 'APGD',
    'patch_pgd': 'Patch-PGD',
    'gn': 'Gaussian Noise',
    'jitter': 'Jitter'
}
results = {}
for atk in attacks:
    if atk in df.columns:
        preds = df[atk].values
        top1 = np.mean(preds == imagenet_labels)
        # Try to get top-5 predictions if available
        if atk+'_top5' in df.columns:
            preds_top5 = df[atk+'_top5'].apply(lambda x: eval(x) if isinstance(x, str) else x)
            preds_top5 = np.stack(preds_top5.values)
            top5 = top5_acc(preds_top5, imagenet_labels)
        else:
            top5 = None
        results[attack_names[atk]] = {'Top-1': top1, 'Top-5': top5}
summary_df = pd.DataFrame(results).T
summary_df['Top-1'] = (summary_df['Top-1'] * 100).round(2)
if summary_df['Top-5'].notnull().all():
    summary_df['Top-5'] = (summary_df['Top-5'] * 100).round(2)
display(summary_df)

"""### Writeup: Task 5: Transferring Attacks

We tested the transferability of the adversarial examples generated on ResNet-34 to a different model, EfficientNet-V2-S, pretrained on ImageNet-1K. This helps assess whether adversarial perturbations remain effective across architectures.

All test sets — original, FGSM, PGD, APGD, and Patch-PGD — were evaluated using the same preprocessing and accuracy metrics (top-1 and top-5).

**EfficientNet-V2-S Accuracy Results:**

| Dataset      | Top-1 Accuracy | Top-5 Accuracy |
|--------------|----------------|----------------|
| Original     | 85.20%         | 98.40%         |
| FGSM         | 69.20%         | 91.60%         |
| PGD          | 77.20%         | 95.00%         |
| APGD         | 79.40%         | 96.40%         |
| Patch-PGD    | 81.60%         | 96.80%         |

**Observations:**
- All attacks caused some drop in accuracy, confirming partial transferability.
- FGSM showed the highest drop in top-1 accuracy (16%), even though it's the simplest method. This suggests it transfers more effectively than stronger attacks like PGD and APGD in some settings.
- PGD and APGD had high transfer robustness on ResNet-34 but less impact on EfficientNet-V2-S.
- Patch-PGD caused the least degradation, likely due to its localized nature.

**Visual Validation:**
We displayed 3 examples from the dataset and visualized how the same image was affected across FGSM, PGD, and Patch-PGD attacks. All adversarial versions appeared visually similar to the original images.

**Conclusion:**
- Adversarial attacks exhibit cross-model transferability, but the effect varies by attack type and target model.
- Simpler attacks like FGSM may generalize better across architectures.
- Stronger attacks (PGD, APGD) may be more model-specific.
- Defenses to mitigate transferability include adversarial training across model ensembles, input randomization, and robust preprocessing pipelines.

## Results / Visualizations
"""

# Cell: Bar Chart (Top-1 and Top-5)
import matplotlib.pyplot as plt

summary_df[['Top-1', 'Top-5']].plot(kind='bar', figsize=(10,6))
plt.ylabel('Accuracy (%)')
plt.title('Top-1 and Top-5 Accuracy of ResNet-34 under Different Attacks')
plt.xticks(rotation=30, ha='right')
plt.tight_layout()
plt.show()

# SIMPLE WORKING CODE - ALL ATTACKS VISUALIZATION
def compare_all_attacks(idx):
    """Compare original image with all implemented attacks."""
    # Get original image
    orig_img, label = orig_dataset[idx]
    true_label = label_to_imagenet[label]

    # Set up figure
    fig, axs = plt.subplots(2, 3, figsize=(15, 10))
    axs = axs.flatten()

    # Original image
    orig_vis = denormalize(orig_img.unsqueeze(0)).squeeze(0).cpu()
    with torch.no_grad():
        orig_pred = pretrained_model(orig_img.unsqueeze(0).to(device)).argmax(dim=1).item()
        orig_pred_name = imagenet_class_mapping.get(orig_pred, str(orig_pred))

    axs[0].imshow(TF.to_pil_image(orig_vis))
    axs[0].set_title(f"Original\nPrediction: {orig_pred_name}", fontsize=12)
    axs[0].axis('off')

    # Add FGSM attack
    try:
        fgsm_img, _ = fgsm_dataset[idx]
        with torch.no_grad():
            fgsm_pred = pretrained_model(fgsm_img.unsqueeze(0).to(device)).argmax(dim=1).item()
            fgsm_pred_name = imagenet_class_mapping.get(fgsm_pred, str(fgsm_pred))

        fgsm_vis = denormalize(fgsm_img.unsqueeze(0)).squeeze(0).cpu()
        axs[1].imshow(TF.to_pil_image(fgsm_vis))
        success = "✓" if fgsm_pred != orig_pred else "✗"
        axs[1].set_title(f"FGSM ({success})\nPrediction: {fgsm_pred_name}", fontsize=12)
        axs[1].axis('off')
    except Exception as e:
        print(f"Error with FGSM: {e}")
        axs[1].text(0.5, 0.5, f"FGSM unavailable", ha='center', va='center', fontsize=12)
        axs[1].axis('off')

    # Add PGD attack
    try:
        pgd_img, _ = pgd_dataset[idx]
        with torch.no_grad():
            pgd_pred = pretrained_model(pgd_img.unsqueeze(0).to(device)).argmax(dim=1).item()
            pgd_pred_name = imagenet_class_mapping.get(pgd_pred, str(pgd_pred))

        pgd_vis = denormalize(pgd_img.unsqueeze(0)).squeeze(0).cpu()
        axs[2].imshow(TF.to_pil_image(pgd_vis))
        success = "✓" if pgd_pred != orig_pred else "✗"
        axs[2].set_title(f"PGD ({success})\nPrediction: {pgd_pred_name}", fontsize=12)
        axs[2].axis('off')
    except Exception as e:
        print(f"Error with PGD: {e}")
        axs[2].text(0.5, 0.5, f"PGD unavailable", ha='center', va='center', fontsize=12)
        axs[2].axis('off')

    # Add APGD attack
    try:
        apgd_img, _ = apgd_dataset[idx]
        with torch.no_grad():
            apgd_pred = pretrained_model(apgd_img.unsqueeze(0).to(device)).argmax(dim=1).item()
            apgd_pred_name = imagenet_class_mapping.get(apgd_pred, str(apgd_pred))

        apgd_vis = denormalize(apgd_img.unsqueeze(0)).squeeze(0).cpu()
        axs[3].imshow(TF.to_pil_image(apgd_vis))
        success = "✓" if apgd_pred != orig_pred else "✗"
        axs[3].set_title(f"APGD ({success})\nPrediction: {apgd_pred_name}", fontsize=12)
        axs[3].axis('off')
    except Exception as e:
        print(f"Error with APGD: {e}")
        axs[3].text(0.5, 0.5, f"APGD unavailable", ha='center', va='center', fontsize=12)
        axs[3].axis('off')

    # Add Patch-PGD attack
    try:
        patch_img, _ = patch_pgd_dataset[idx]
        with torch.no_grad():
            patch_pred = pretrained_model(patch_img.unsqueeze(0).to(device)).argmax(dim=1).item()
            patch_pred_name = imagenet_class_mapping.get(patch_pred, str(patch_pred))

        patch_vis = denormalize(patch_img.unsqueeze(0)).squeeze(0).cpu()
        axs[4].imshow(TF.to_pil_image(patch_vis))
        success = "✓" if patch_pred != orig_pred else "✗"
        axs[4].set_title(f"Patch-PGD ({success})\nPrediction: {patch_pred_name}", fontsize=12)
        axs[4].axis('off')
    except Exception as e:
        print(f"Error with Patch-PGD: {e}")
        axs[4].text(0.5, 0.5, f"Patch-PGD unavailable", ha='center', va='center', fontsize=12)
        axs[4].axis('off')

    # Add GN attack
    try:
        gn_img, _ = gn_dataset[idx]
        with torch.no_grad():
            gn_pred = pretrained_model(gn_img.unsqueeze(0).to(device)).argmax(dim=1).item()
            gn_pred_name = imagenet_class_mapping.get(gn_pred, str(gn_pred))

        gn_vis = denormalize(gn_img.unsqueeze(0)).squeeze(0).cpu()
        axs[5].imshow(TF.to_pil_image(gn_vis))
        success = "✓" if gn_pred != orig_pred else "✗"
        axs[5].set_title(f"GN ({success})\nPrediction: {gn_pred_name}", fontsize=12)
        axs[5].axis('off')
    except Exception as e:
        print(f"Error with GN: {e}")
        axs[5].text(0.5, 0.5, f"GN unavailable", ha='center', va='center', fontsize=12)
        axs[5].axis('off')

    true_label_name = imagenet_class_mapping.get(true_label, str(true_label))
    plt.suptitle(f"True Label: {true_label_name}\nComparison of Different Attacks", fontsize=14)

    plt.tight_layout()
    return fig

# Find 3 examples that are correctly classified originally
successful_indices = []
for idx, row in df.iterrows():
    img, label = orig_dataset[idx]
    true_label = label_to_imagenet[label]
    if row['orig'] == true_label:
        successful_indices.append(idx)
        if len(successful_indices) >= 3:
            break

# Show examples
for idx in successful_indices:
    fig = compare_all_attacks(idx)
    plt.savefig(f"all_attacks_comparison_{idx}.png", dpi=300, bbox_inches='tight')
    plt.show()

"""### Results: Visual Comparisons and Accuracy Trends

To complement the quantitative accuracy metrics, we include additional visualizations to better understand the impact of different adversarial attacks.

**1. Accuracy Bar Chart:**
- We plotted top-1 and top-5 accuracy of ResNet-34 across all attack types (FGSM, PGD, APGD, Patch-PGD, Gaussian Noise, and Jitter).
- The chart confirms that iterative attacks like PGD and APGD lead to the sharpest drops in accuracy.
- Gaussian Noise has minimal effect on the model, as expected, while Jitter introduces moderate degradation.

**2. Visual Examples – Attack Effectiveness:**
- We selected 3 test images that the model correctly classifies in the clean/original form.
- For each image, we visualized:
  - The original prediction
  - Predictions after applying FGSM, PGD, APGD, Patch-PGD, and Gaussian Noise
- A ✓ or ✗ symbol indicates whether the attack caused a misclassification.
- These side-by-side comparisons make it clear which attacks were successful and how resilient or vulnerable the model is on a per-image basis.

**3. Perceptual Similarity:**
- All adversarial examples remain visually indistinguishable from the original images to a human observer.
- Despite this, the classifier’s predictions change significantly — showing how small perturbations can have large semantic consequences for the model.

**4. Summary Table:**
- We generated a summary table from all prediction results to report exact top-1 and top-5 accuracy across methods.
- This helped validate that the performance drop from each attack exceeded the required threshold set in the project.

These results reinforce that even subtle, imperceptible pixel changes — when guided by gradient-based methods — can reliably fool modern deep neural networks.
"""

# Step-by-step in one cell: move -> zip -> download
import os
from google.colab import files

# Create export folder
os.makedirs('/content/final_export', exist_ok=True)

# Move relevant files/folders
!mv /content/AdversarialTestSet* /content/final_export/ 2>/dev/null
!mv /content/TestDataSet /content/final_export/ 2>/dev/null
!mv /content/pred.csv /content/final_export/ 2>/dev/null
!mv /content/all_attacks_comparison_*.png /content/final_export/ 2>/dev/null

# Zip the folder
!zip -r /content/final_submission.zip /content/final_export > /dev/null

# Download the zip
files.download('/content/final_submission.zip')

